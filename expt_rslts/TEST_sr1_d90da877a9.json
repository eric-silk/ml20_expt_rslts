{"specs": {"opt": "sr1", "dataset": "cifar10", "batch_size_train": 50000, "batch_size_test": 10000, "momentum": 0.0, "hidden": 15, "max_newton": 1, "abs_newton_tol": 0.0001, "rel_newton_tol": 1e-06, "max_cr": 50, "cr_tol": 0.01, "learning_rate": 1.0, "sufficient_decrease": 0.001, "curvature_condition": null, "extrapolation_factor": 0.9, "max_searches": 10, "num_epoch": 20, "seed": 10, "read_nn": null, "write_nn": true, "log_interval": 10, "device": "cuda", "record": "./expt_rslts/", "memory": 2}, "time": [10.85739665478468, 10.296025473624468, 5.918240597471595, 5.9232626017183065, 10.825232449918985, 10.827721364796162, 5.92784153483808, 5.946073779836297, 10.825837885960937, 5.907724255695939, 5.936089988797903, 10.802769964560866, 5.926608752459288, 5.917356915771961, 10.833486968651414, 6.009045384824276, 5.9441396705806255, 10.887298919260502, 5.952620763331652, 5.981951579451561], "train_loss_list": [6.843976020812988, 6.841654300689697, 6.63823127746582, 4.813277721405029, 5.247411251068115, 5.528864860534668, 4.1321892738342285, 3.951449155807495, 4.135012626647949, 4.076957702636719, 4.050043106079102, 4.115699291229248, 4.077577114105225, 4.059929847717285, 4.095724582672119, 4.0777764320373535, 4.0649824142456055, 4.085474491119385, 4.078173637390137, 4.067113399505615], "test_loss_list": [0.00035694282054901124, 0.0005200023651123046, 0.0022352285385131834, 0.0007328564167022705, 0.0005562733650207519, 0.0005581873416900634, 0.0005249913692474365, 0.0004319401264190674, 0.0004156259536743164, 0.0004218399524688721, 0.0004151894092559814, 0.00041496472358703613, 0.0004096481800079346, 0.0004092247486114502, 0.0004122652530670166, 0.0004110674381256104, 0.0004097562313079834, 0.00041171317100524904, 0.000413527250289917, 0.00041157841682434084], "test_accuracy_list": [10.13, 16.26, 10.2, 17.25, 17.05, 14.72, 14.24, 14.51, 15.01, 15.28, 15.3, 15.21, 15.28, 15.38, 15.13, 15.4, 15.43, 15.15, 15.45, 15.4], "train_accuracy_list": [15.284, 17.328, 11.018, 16.362, 17.054, 14.716, 15.65, 15.828, 15.332, 15.702, 15.768, 15.716, 15.858, 15.894, 15.752, 15.898, 15.928, 15.742, 15.932, 15.926]}